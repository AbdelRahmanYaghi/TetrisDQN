{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from core import game\n",
    "import random\n",
    "from env import TetrisEnv\n",
    "import numpy as np\n",
    "import gym\n",
    "from keras.layers import Conv2D,Dense, Flatten, Input, concatenate, Reshape, MaxPooling2D\n",
    "from keras.models import Sequential \n",
    "from keras.optimizers import Adam\n",
    "from keras.models import Model\n",
    "import keras\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime\n",
    "from IPython.display import display, clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 42\n",
    "discount_factor = 0.95  # Discount factor for past rewards\n",
    "epsilon = 1.0  # Epsilon greedy parameter\n",
    "epsilon_min = 0.01  # Minimum epsilon greedy parameter\n",
    "epsilon_max = 1.0  # Maximum epsilon greedy parameter\n",
    "epsilon_interval = 0.993  # Rate at which to reduce chance of random action being taken\n",
    "batch_size = 64  # Size of batch taken from replay buffer\n",
    "max_steps_per_episode = 500000\n",
    "\n",
    "# Use the Baseline Atari environment because of Deepmind helper functions\n",
    "env = TetrisEnv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_action_loc = 12\n",
    "num_action_rot = 4\n",
    "\n",
    "def create_q_model():\n",
    "    input_all = Input(shape=(4), name = 'Input')\n",
    "\n",
    "    x1 = Dense(32, activation='relu')(input_all)\n",
    "    x1 = Dense(32, activation='relu')(x1)\n",
    "\n",
    "    output = Dense(1, activation='linear')(x1)\n",
    "\n",
    "    model = Model(inputs=input_all, outputs=output)\n",
    "\n",
    "    return model\n",
    "\n",
    "q_model = create_q_model()\n",
    "\n",
    "optimizer = keras.optimizers.Adam(learning_rate=1e-3, clipnorm=1.0)\n",
    "\n",
    "# Experience replay buffers\n",
    "current_state_memory = []\n",
    "next_state_memory = []\n",
    "reward_memory = []\n",
    "done_memory = []\n",
    "\n",
    "cumm_reward_history = []\n",
    "\n",
    "episode_train_interval = 2\n",
    "\n",
    "episode_count = 0\n",
    "frame_count = 0\n",
    "# Number of frames to take random action and observe output\n",
    "epsilon_random_frames = 1\n",
    "# Number of frames for exploration\n",
    "epsilon_greedy_frames = 10000\n",
    "# Train the model after 4 actions\n",
    "update_after_actions = 4\n",
    "\n",
    "q_model.compile(loss = 'mse', optimizer='Adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4]\n",
      "2\n",
      "[4]\n",
      "2\n",
      "[1]\n",
      "15\n",
      "[1]\n",
      "15\n",
      "[5]\n",
      "11\n",
      "[5]\n",
      "11\n",
      "[7]\n",
      "9\n",
      "[7]\n",
      "9\n",
      "[1]\n",
      "15\n",
      "[1]\n",
      "15\n",
      "[3]\n",
      "12\n",
      "[3]\n",
      "12\n",
      "[4]\n",
      "4\n",
      "[4]\n",
      "4\n",
      "[7]\n",
      "9\n",
      "[7]\n",
      "9\n",
      "[5]\n",
      "12\n",
      "[5]\n",
      "12\n",
      "[6]\n",
      "23\n",
      "[6]\n",
      "23\n",
      "[7]\n",
      "12\n",
      "[7]\n",
      "12\n",
      "[2]\n",
      "28\n",
      "[2]\n",
      "28\n",
      "[1]\n",
      "25\n",
      "[1]\n",
      "25\n",
      "[5]\n",
      "15\n",
      "[5]\n",
      "15\n",
      "[6]\n",
      "23\n",
      "[6]\n",
      "23\n",
      "[2]\n",
      "21\n",
      "[2]\n",
      "21\n",
      "[1]\n",
      "16\n",
      "[1]\n",
      "16\n",
      "[6]\n",
      "18\n",
      "[6]\n",
      "18\n",
      "[6]\n",
      "20\n",
      "[6]\n",
      "20\n",
      "[5]\n",
      "12\n",
      "[5]\n",
      "12\n",
      "[6]\n",
      "20\n",
      "[6]\n",
      "20\n",
      "[4]\n",
      "5\n",
      "[4]\n",
      "5\n",
      "[6]\n",
      "19\n",
      "[6]\n",
      "19\n",
      "[3]\n",
      "12\n",
      "[3]\n",
      "12\n",
      "[2]\n",
      "22\n",
      "[2]\n",
      "22\n",
      "[3]\n",
      "12\n",
      "[3]\n",
      "12\n",
      "[5]\n",
      "15\n",
      "[5]\n",
      "15\n",
      "[7]\n",
      "10\n",
      "[7]\n",
      "10\n",
      "[5]\n",
      "14\n",
      "[5]\n",
      "14\n",
      "[1]\n",
      "18\n",
      "[1]\n",
      "18\n",
      "[7]\n",
      "10\n",
      "[7]\n",
      "10\n",
      "[1]\n",
      "7\n",
      "[1]\n",
      "7\n",
      "[2]\n",
      "15\n",
      "[2]\n",
      "15\n",
      "[3]\n",
      "9\n",
      "[3]\n",
      "9\n",
      "[4]\n",
      "5\n",
      "[4]\n",
      "5\n",
      "[7]\n",
      "11\n",
      "[7]\n",
      "11\n",
      "[3]\n",
      "9\n",
      "[3]\n",
      "9\n",
      "[2]\n",
      "19\n",
      "[2]\n",
      "19\n",
      "[7]\n",
      "9\n",
      "[7]\n",
      "9\n",
      "[3]\n",
      "12\n",
      "[3]\n",
      "12\n",
      "[2]\n",
      "21\n",
      "[2]\n",
      "21\n",
      "[6]\n",
      "23\n",
      "[6]\n",
      "23\n",
      "[4]\n",
      "7\n",
      "[4]\n",
      "7\n",
      "[2]\n",
      "22\n",
      "[2]\n",
      "22\n",
      "[5]\n",
      "11\n",
      "[5]\n",
      "11\n",
      "[6]\n",
      "16\n",
      "[6]\n",
      "16\n",
      "[3]\n",
      "11\n",
      "[3]\n",
      "11\n",
      "[4]\n",
      "6\n",
      "[4]\n",
      "6\n",
      "[2]\n",
      "20\n",
      "[2]\n",
      "20\n",
      "[5]\n",
      "7\n",
      "[5]\n",
      "7\n",
      "[6]\n",
      "22\n",
      "[6]\n",
      "22\n",
      "[1]\n",
      "22\n",
      "[1]\n",
      "22\n",
      "[3]\n",
      "10\n",
      "[3]\n",
      "10\n",
      "[7]\n",
      "11\n",
      "[7]\n",
      "11\n",
      "[5]\n",
      "8\n",
      "[5]\n",
      "8\n",
      "[4]\n",
      "5\n",
      "[4]\n",
      "5\n",
      "[7]\n",
      "12\n",
      "[7]\n",
      "12\n",
      "[6]\n",
      "19\n",
      "[6]\n",
      "19\n",
      "[5]\n",
      "11\n",
      "[5]\n",
      "11\n",
      "[7]\n",
      "10\n",
      "[7]\n",
      "10\n",
      "[4]\n",
      "6\n",
      "[4]\n",
      "6\n",
      "[7]\n",
      "11\n",
      "[7]\n",
      "11\n",
      "[5]\n",
      "9\n",
      "[5]\n",
      "9\n",
      "[6]\n",
      "17\n",
      "[6]\n",
      "17\n",
      "[7]\n",
      "11\n",
      "[7]\n",
      "11\n",
      "[3]\n",
      "11\n",
      "[3]\n",
      "11\n",
      "[7]\n",
      "11\n",
      "[7]\n",
      "11\n",
      "[6]\n",
      "25\n",
      "[6]\n",
      "25\n",
      "[5]\n",
      "11\n",
      "[5]\n",
      "11\n",
      "[1]\n",
      "23\n",
      "[1]\n",
      "23\n",
      "[5]\n",
      "11\n",
      "[5]\n",
      "11\n",
      "[2]\n",
      "15\n",
      "[2]\n",
      "15\n",
      "[6]\n",
      "19\n",
      "[6]\n",
      "19\n",
      "[1]\n",
      "16\n",
      "[1]\n",
      "16\n",
      "[6]\n",
      "20\n",
      "[6]\n",
      "20\n",
      "[7]\n",
      "7\n",
      "[7]\n",
      "7\n",
      "[2]\n",
      "14\n",
      "[2]\n",
      "14\n",
      "[1]\n",
      "15\n",
      "[1]\n",
      "15\n",
      "[6]\n",
      "18\n",
      "[6]\n",
      "18\n",
      "[1]\n",
      "16\n",
      "[1]\n",
      "16\n",
      "[3]\n",
      "9\n",
      "[3]\n",
      "9\n",
      "[5]\n",
      "12\n",
      "[5]\n",
      "12\n",
      "[4]\n",
      "6\n",
      "[4]\n",
      "6\n",
      "[6]\n",
      "19\n",
      "[6]\n",
      "19\n",
      "[5]\n",
      "14\n",
      "[5]\n",
      "14\n",
      "[2]\n",
      "9\n",
      "[2]\n",
      "9\n",
      "[7]\n",
      "9\n",
      "[7]\n",
      "9\n",
      "[4]\n",
      "4\n",
      "[4]\n",
      "4\n",
      "[6]\n",
      "16\n",
      "[6]\n",
      "16\n",
      "[5]\n",
      "12\n",
      "[5]\n",
      "12\n",
      "[1]\n",
      "18\n",
      "[1]\n",
      "18\n",
      "[7]\n",
      "10\n",
      "[7]\n",
      "10\n",
      "[2]\n",
      "18\n",
      "[2]\n",
      "18\n",
      "[6]\n",
      "18\n",
      "[6]\n",
      "18\n",
      "[7]\n",
      "9\n",
      "[7]\n",
      "9\n",
      "[7]\n",
      "9\n",
      "[7]\n",
      "9\n",
      "[1]\n",
      "18\n",
      "[1]\n",
      "18\n",
      "[7]\n",
      "10\n",
      "[7]\n",
      "10\n",
      "[3]\n",
      "14\n",
      "[3]\n",
      "14\n",
      "[4]\n",
      "6\n",
      "[4]\n",
      "6\n",
      "[5]\n",
      "10\n",
      "[5]\n",
      "10\n",
      "[6]\n",
      "18\n",
      "[6]\n",
      "18\n",
      "[1]\n",
      "21\n",
      "[1]\n",
      "21\n",
      "[2]\n",
      "19\n",
      "[2]\n",
      "19\n",
      "[7]\n",
      "10\n",
      "[7]\n",
      "10\n",
      "[3]\n",
      "8\n",
      "[3]\n",
      "8\n",
      "[2]\n",
      "18\n",
      "[2]\n",
      "18\n",
      "[7]\n",
      "10\n",
      "[7]\n",
      "10\n",
      "[3]\n",
      "11\n",
      "[3]\n",
      "11\n",
      "[5]\n",
      "14\n",
      "[5]\n",
      "14\n",
      "[1]\n",
      "15\n",
      "[1]\n",
      "15\n",
      "[5]\n",
      "13\n",
      "[5]\n",
      "13\n",
      "[7]\n",
      "11\n",
      "[7]\n",
      "11\n",
      "[3]\n",
      "13\n",
      "[3]\n",
      "13\n",
      "[4]\n",
      "6\n",
      "[4]\n",
      "6\n",
      "[6]\n",
      "21\n",
      "[6]\n",
      "21\n",
      "[4]\n",
      "7\n",
      "[4]\n",
      "7\n",
      "[2]\n",
      "23\n",
      "[2]\n",
      "23\n",
      "[6]\n",
      "23\n",
      "[6]\n",
      "23\n",
      "[3]\n",
      "10\n",
      "[3]\n",
      "10\n",
      "[1]\n",
      "22\n",
      "[1]\n",
      "22\n",
      "[7]\n",
      "12\n",
      "[7]\n",
      "12\n",
      "[4]\n",
      "8\n",
      "[4]\n",
      "8\n",
      "[7]\n",
      "9\n",
      "[7]\n",
      "9\n",
      "[5]\n",
      "13\n",
      "[5]\n",
      "13\n",
      "[6]\n",
      "21\n",
      "[6]\n",
      "21\n",
      "[4]\n",
      "4\n",
      "[4]\n",
      "4\n",
      "[7]\n",
      "9\n",
      "[7]\n",
      "9\n",
      "Solved at episode 3!\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "time.sleep(1)\n",
    "epsilon = -1\n",
    "q_model.load_weights('Model_Weights.h5')\n",
    "\n",
    "while True: \n",
    "    state = env.reset()\n",
    "    cumm_reward = 0\n",
    "    done = False\n",
    "    while not done:\n",
    "        frame_count += 1\n",
    "\n",
    "        next_possible_states = np.array([np.array(i) for i in env.game.getNextStates().keys()])\n",
    "\n",
    "        if np.random.uniform() < epsilon:\n",
    "            best_move = next_possible_states[np.random.choice(next_possible_states.shape[0], size=1), :].reshape(4)\n",
    "        else:\n",
    "            best_value = None\n",
    "            for state in next_possible_states:\n",
    "                value = q_model.predict(state.reshape(1, 4), verbose = 0)\n",
    "                if not best_value or value > best_value:\n",
    "                    best_value = value\n",
    "                    best_move = np.array(state).reshape(4)\n",
    "\n",
    "        best_action = env.game.getNextStates()[tuple(best_move.flatten())]\n",
    "        old_pg = env.game.playground\n",
    "        next_state, reward, done, _  = env.step(best_action)\n",
    "        new_pg = env.game.playground\n",
    "        \n",
    "\n",
    "        cumm_reward += reward\n",
    "\n",
    "        current_state_memory.append(state)\n",
    "        next_state_memory.append(next_state)\n",
    "        reward_memory.append(reward)\n",
    "        done_memory.append(done)        \n",
    "\n",
    "        state = next_state\n",
    "\n",
    "    cumm_reward_history.append(cumm_reward)\n",
    "    episode_count += 1\n",
    "    # if episode_count % episode_train_interval == 0:\n",
    "    #     x = []\n",
    "    #     y = []\n",
    "\n",
    "    #     batch_current_memory = current_state_memory[-batch_size:]\n",
    "    #     batch_next_memory = next_state_memory[-batch_size:]\n",
    "    #     batch_reward_memory = reward_memory[-batch_size:]\n",
    "    #     batch_done_memory = done_memory[-batch_size:]\n",
    "\n",
    "    #     next_predicted_q_values = q_model.predict(np.array(batch_next_memory))\n",
    "    #     for current, reward, done, next_q in zip(batch_current_memory, batch_reward_memory, batch_done_memory, next_predicted_q_values):\n",
    "    #         if not done:\n",
    "    #             new_q_value = (reward + discount_factor * next_q)[0]\n",
    "    #         else:\n",
    "    #             new_q_value = reward\n",
    "\n",
    "    #         x.append(current)\n",
    "    #         y.append(new_q_value)\n",
    "\n",
    "    #     q_model.fit(np.array(x), np.array(y), batch_size=batch_size)\n",
    "\n",
    "    #     if epsilon > epsilon_min:\n",
    "    #         epsilon *= epsilon_interval\n",
    "\n",
    "    if episode_count == 0:\n",
    "        path = r\"../Tetris AI/Outputs\"\n",
    "        max_file = max([int(i[:-4]) for i in os.listdir(path)]) + 1\n",
    "        f = open(\"..\\\\Tetris AI\\\\Outputs\\\\%s.txt\" % max_file, \"w\")\n",
    "    else:\n",
    "        path = r\"../Tetris AI/Outputs\"\n",
    "        curr_file = max([int(i[:-4]) for i in os.listdir(path)])\n",
    "        f = open(\"..\\\\Tetris AI\\\\Outputs\\\\%s.txt\" % curr_file, \"a\")\n",
    "\n",
    "    f.write(\"Currently on episode: %s with reward %s\\n\"%(episode_count, cumm_reward))\n",
    "\n",
    "    f.close()\n",
    "\n",
    "    if (len(current_state_memory) > 512):\n",
    "        current_state_memory = current_state_memory[-512:]\n",
    "        next_state_memory = next_state_memory[-512:]\n",
    "        reward_memory = reward_memory[-512:]\n",
    "        done_memory = done_memory[-512:]\n",
    "\n",
    "    if cumm_reward > 10000:  # Condition to consider the task solved\n",
    "        print(\"Solved at episode {}!\".format(episode_count))\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "84"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7db1d8a7f7d5bd9b5eaf5b8ee94693949889b84862d2647ce6a7479d475e0ae2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
