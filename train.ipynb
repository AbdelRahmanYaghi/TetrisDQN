{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from core import game\n",
    "import random\n",
    "from Virtual_env import Virtual_Env\n",
    "import numpy as np\n",
    "import gym\n",
    "from keras.layers import Conv2D,Dense, Flatten, Input, concatenate, Reshape, MaxPooling2D, Dropout, LayerNormalization\n",
    "from keras.models import Sequential \n",
    "from keras.optimizers import Adam, SGD\n",
    "from keras.models import Model\n",
    "import keras\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime\n",
    "from IPython.display import display, clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "discount_factor = 0.95 \n",
    "epsilon = 1 \n",
    "epsilon_min = 0.01 \n",
    "epsilon_interval = 0.99  \n",
    "batch_size = 512  \n",
    "\n",
    "env = Virtual_Env()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_q_model():\n",
    "    input_all = Input(shape=(4), name = 'Input')\n",
    "\n",
    "    # x1 = LayerNormalization()()\n",
    "    x1 = Dense(32, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.01))(input_all)\n",
    "    # x1 = Dense(8, activation='relu')(x1)\n",
    "    x1 = Dense(32, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.01))(x1)\n",
    "\n",
    "    output = Dense(1, activation='linear')(x1)\n",
    "\n",
    "    model = Model(inputs=input_all, outputs=output)\n",
    "\n",
    "    return model\n",
    "\n",
    "q_model = create_q_model()\n",
    "\n",
    "current_state_memory = []\n",
    "next_state_memory = []\n",
    "reward_memory = []\n",
    "done_memory = []\n",
    "cumm_reward_history = []\n",
    "\n",
    "episode_train_interval = 4\n",
    "\n",
    "episode_count = 0\n",
    "frame_count = 0\n",
    "\n",
    "q_model.compile(loss = 'mse', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_model.load_weights(r'C:\\Users\\Glazed\\Documents\\Root\\Python\\Tetris AI\\Model_Weights.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "time.sleep(1)\n",
    "\n",
    "while True: \n",
    "    state = env.reset()\n",
    "\n",
    "    cumm_reward = 0\n",
    "    done = False\n",
    "    while not done:\n",
    "        frame_count += 1\n",
    "\n",
    "        next_possible_states = np.array([np.array(i) for i in env.getNextStates(env.piece).keys()])\n",
    "\n",
    "        if np.random.uniform() < epsilon:\n",
    "            best_move = next_possible_states[np.random.choice(next_possible_states.shape[0], size=1), :].reshape((20,10))\n",
    "        else:\n",
    "            best_value = None\n",
    "            for poss_state in next_possible_states:\n",
    "                value = q_model.predict(env.pg2state(poss_state.reshape((20, 10))).reshape(1, 4), verbose = 0)\n",
    "                if not best_value or value > best_value:\n",
    "                    best_value = value\n",
    "                    best_move = poss_state\n",
    "\n",
    "        best_action = env.getNextStates(env.piece)[tuple(best_move.flatten())]\n",
    "        old_pg = env.playground\n",
    "        next_state, reward, done, _  = env.step(best_action)\n",
    "\n",
    "        new_pg = env.playground\n",
    "        cumm_reward += reward\n",
    "\n",
    "\n",
    "        current_state_memory.append(state)\n",
    "        next_state_memory.append(next_state)\n",
    "        reward_memory.append(reward)\n",
    "        done_memory.append(done)        \n",
    "\n",
    "        state = next_state\n",
    "\n",
    "    cumm_reward_history.append(cumm_reward)\n",
    "    episode_count += 1\n",
    "    print(\"Currently on episode: %s with reward %s\\n\"%(episode_count, cumm_reward))\n",
    "    if episode_count % episode_train_interval == 0:\n",
    "        x = []\n",
    "        y = []\n",
    "\n",
    "        if len(current_state_memory) < batch_size:\n",
    "            amount_to_sample = len(current_state_memory)\n",
    "        else:\n",
    "            amount_to_sample = batch_size\n",
    "\n",
    "        random_batch = np.random.choice(len(current_state_memory),amount_to_sample , replace=False)\n",
    "\n",
    "        batch_current_memory = np.array(current_state_memory)[random_batch]\n",
    "        batch_next_memory = np.array(next_state_memory)[random_batch]\n",
    "        batch_reward_memory = np.array(reward_memory)[random_batch]\n",
    "        batch_done_memory = np.array(done_memory)[random_batch]\n",
    "\n",
    "        next_predicted_q_values = q_model.predict(np.array(batch_next_memory))\n",
    "        for current, reward, done, next_q in zip(batch_current_memory, batch_reward_memory, batch_done_memory, next_predicted_q_values):\n",
    "            if not done:\n",
    "                new_q_value = (reward + discount_factor * next_q)[0]\n",
    "            else:\n",
    "                new_q_value = reward\n",
    "\n",
    "            x.append(current)\n",
    "            y.append(new_q_value)\n",
    "\n",
    "        q_model.fit(np.array(x), np.array(y), batch_size=batch_size)\n",
    "\n",
    "        if epsilon > epsilon_min:\n",
    "            epsilon -= epsilon_interval\n",
    "\n",
    "    keras.backend.clear_session()\n",
    "    q_model.reset_states()\n",
    "\n",
    "    if (len(current_state_memory) > 8192):\n",
    "        del current_state_memory[0]\n",
    "        del next_state_memory[0]\n",
    "        del reward_memory[0]\n",
    "        del done_memory[0]\n",
    "\n",
    "    if len(cumm_reward_history) > 8192:\n",
    "        del cumm_reward_history[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_model.save_weights('Model_Weights.h5')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7db1d8a7f7d5bd9b5eaf5b8ee94693949889b84862d2647ce6a7479d475e0ae2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
